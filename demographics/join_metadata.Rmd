---
title: "Join metadata with GCP files"
author: "Bria Long"
date: "2024-10-25"
output: html_document
---

# Setup
```{r}
library(tidyverse)
library(here)
library(googlesheets4)
library(lubridate)
library(ggthemes)
library(knitr)
```

# Load family demographics
IDENTIFIABLE (!) with birthdate if you get it from the source; that bit is commented out now.

Here we are using a deidentified version to check rough ages
```{r}
families = read_csv(file=here::here('data/demographics_2025.csv')) %>%
  as_tibble() %>%
  filter(study_name == 'BabyView') %>% # no pilots
  select(sid, date_birth, ethnicity, gender, num_lang, parent_ed) %>%
  rename(birthdate = date_birth) %>%
  mutate(subject_id = paste0('00',sid)) %>%
  filter(!is.na(subject_id))

# We collected data from `r length(unique(families$subject_id))`.
# In our sample, `r sum(families$num_lang>1)` children are exposed to more than one language.

families_short <- families %>%
  select(subject_id, birthdate, num_lang) %>%
  mutate(num_lang = as.numeric(num_lang)) 

```

```{r}
# families_deidentified <- families %>%
  # select(subject_id, birthdate, num_lang) %>%
  # mutate(birthdate = mdy(birthdate)) %>%
  # mutate(birthdate = round_date(birthdate, unit="month")) 

# write_csv(families_deidentified, file=here::here('data/release_2.0/subids_deidentified.csv'))
```

Load in deidentified info
```{r}
# families_short = read_csv(file=('subids_deidentified.csv'))  
```

## Load bv main -- ongoing 
```{R}
# ongoing data collection
ongoing_session_durations <- read_sheet('https://docs.google.com/spreadsheets/d/1mAti9dBNUqgNQQIIsnPb5Hu59ovKCUh9LSYOcQvzt2U/edit?gid=754020357#gid=754020357',sheet='Ongoing_data_collection') %>%
  filter(Status=="Uploaded") %>%
  select(-Notes) %>% # causing join errors because incompatible types
  select(subject_id, video_id,  Upload_fname, Date, `Blackout Portions`, Duration) %>%
  rename(exclude = `Blackout Portions`) %>%
  left_join(families_short, by=c('subject_id')) %>%
  mutate(cohort = 'ongoing')  %>%
  mutate(video_name = str_split_fixed(Upload_fname, '.zip',2)[,1]) %>%
  # Fix the date column, which is in a list for this sheet but not all
  filter(map_lgl(Date, ~ !is.null(.x)))  %>%
  mutate(date_column = map_chr(Date, ~ as.character(.x[1])))  %>%
  mutate(date_tested = ymd(date_column))  %>%
  select(-Date, -date_column) %>%
  mutate(video_name = str_replace_all(video_name, "PM", "pm"),
         video_name = str_replace_all(video_name, "AM", "am")) 

## Not missing video names
# sum(is.na(ongoing_session_durations$Upload_fname))

# Missing dates?
sum(is.na(ongoing_session_durations$date_tested))
```


## Load bv main -- release 1 
```{r}
# release 1
release_1_session_durations <- read_sheet('https://docs.google.com/spreadsheets/d/1mAti9dBNUqgNQQIIsnPb5Hu59ovKCUh9LSYOcQvzt2U/edit?gid=1883822719#gid=1883822719', sheet='Main_Release_1_Corrected_test') %>%
  filter(Vid_In_Storage_Bucket=="Y") %>% # filter to uploadeed vids
  select(subject_id, video_id, old_name, Date, `Blackout Portions`, Duration, `grace notes`) %>%
  rename(exclude = `Blackout Portions`) %>% # tag when there was anything to be excluded
  rename(changed = `grace notes`) %>%
  left_join(families_short, by=c('subject_id')) %>%
  mutate(cohort = 'release_1') %>%
  mutate(low_res = str_detect(old_name,'.LRV')) %>%
  mutate(video_name = str_split_fixed(old_name, "\\.MP4|\\.LRV|\\.ZIP", n = 2)[, 1]) %>%
  # Fix the date column, which is in a list for this one
  mutate(date_tested = mdy(Date)) %>%
  mutate(Duration = as.numeric(Duration)) %>%
  mutate(video_name = str_replace_all(video_name, "PM", "pm"),
         video_name = str_replace_all(video_name, "AM", "am")) 

## Stil missing 1090 video names as of 10pm on 2/10/25
sum(is.na(release_1_session_durations$video_name))
sum(is.na(release_1_session_durations$date_tested))

```

## Ego-single child, releases 1/2
```{r}
# release 1
luna_release_1_session_durations <- read_sheet('https://docs.google.com/spreadsheets/d/1mAti9dBNUqgNQQIIsnPb5Hu59ovKCUh9LSYOcQvzt2U/edit?gid=1883822719#gid=1883822719', sheet='Luna_V1_Corrected') %>%
  filter(Vid_In_Storage_Bucket=="Y") %>%
  select(subject_id, video_id, Upload_fname, Date, `Blackout Portions`, Duration) %>%
  rename(exclude = `Blackout Portions`) %>% # tag when there was anything to be excluded
  left_join(families_short, by=c('subject_id')) %>%
  mutate(cohort = 'ego_single') %>%
  mutate(video_name = str_split_fixed(Upload_fname, '.zip',2)[,1]) %>%
  # Fix the date column, which is in a list for this one
  filter(map_lgl(Date, ~ !is.null(.x)))  %>%
  mutate(date_column = map_chr(Date, ~ as.character(.x[1])))  %>%
  mutate(date_tested = ymd(date_column))  %>%
  mutate(Duration = as.numeric(Duration)) %>%  # weird in `test` sheet
  select(-Date)

## Not missing luna names
sum(is.na(luna_release_1_session_durations$Upload_fname))
sum(is.na(luna_release_1_session_durations$date_tested))

```

```{r}
# release 2 luna
luna_release_2_session_durations <- read_sheet('https://docs.google.com/spreadsheets/d/1mAti9dBNUqgNQQIIsnPb5Hu59ovKCUh9LSYOcQvzt2U/edit?gid=1883822719#gid=1883822719', sheet='Luna_Round_2_Ongoing') %>%
  filter(Status=="Uploaded") %>%
  select(subject_id, video_id, Upload_fname, Date, `Delete?`, Duration) %>%
  rename(exclude = `Delete?`) %>% # tag when there was anything to be excluded
  left_join(families_short, by=c('subject_id')) %>%
  mutate(cohort = 'ego_single') %>%
  mutate(video_name = str_split_fixed(Upload_fname, '.zip',2)[,1]) %>%
  # Fix the date column
  mutate(date_column = ymd(Date)) %>%
  mutate(date_tested = ymd(date_column))  %>%
  select(-Date, -date_column)

sum(is.na(luna_release_2_session_durations$Upload_fname))


```

```{r}
# all bing
bing_sessions <- read_sheet('https://docs.google.com/spreadsheets/d/1mAti9dBNUqgNQQIIsnPb5Hu59ovKCUh9LSYOcQvzt2U/edit?gid=1883822719#gid=1883822719', sheet='Bing') %>%
  filter(Vid_In_Storage_Bucket=="Y") %>%
  select(subject_id, video_id, Upload_fname, Date, `Blackout Portions`, Duration) %>%
  rename(exclude = `Blackout Portions`) %>% # tag when there was anything to be excluded
  left_join(families_short, by=c('subject_id')) %>%
  mutate(cohort = 'bing') %>%
  mutate(video_name = str_split_fixed(Upload_fname, '.zip',2)[,1]) %>%
  # Fix the date column, which is in a list for this one
  filter(map_lgl(Date, ~ !is.null(.x)))  %>%
  mutate(date_column = map_chr(Date, ~ as.character(.x[1])))  %>%
  mutate(date_tested = ymd(date_column))   %>%
  select(-Date, -date_column)

## Not missing bing names
sum(is.na(bing_sessions$Upload_fname))

```


## Join all together
```{r}
all_sessions <- release_1_session_durations %>% 
  full_join(ongoing_session_durations) %>%
  full_join(luna_release_1_session_durations) %>%
  full_join(luna_release_2_session_durations) %>%
  full_join(bing_sessions) 
  
```


# Join metadata with filenames from GCP pulls
```{R}
to_join <- all_sessions %>%
  arrange(video_name) %>%
  mutate(filename = video_name) 
```

Here's the file list from Khais' pull on GCP, try to join


```{r}
joined_release_2 <- read_csv(file = here::here('data/included_videos.csv'))   %>%
  mutate(filename = str_replace_all(filename, regex("n\\.a", ignore_case = TRUE), "NA")) %>%
  mutate(filename = str_replace_all(filename, regex("\\bna\\b", ignore_case = TRUE), "NA")) %>%
  mutate(filename = str_replace_all(filename, "PM", "pm"),
         filename = str_replace_all(filename, "AM", "am")) %>%
  left_join(to_join)
```

Get date information from week range
```{r}
wrong_dates <- joined_release_2 %>%
  filter(date_tested < ymd("2019-01-01")) %>%
  mutate(date_tested = NA)

wrong_dates_2 <- joined_release_2 %>%
  filter(date_tested > ymd("2027-01-01")) %>%
  mutate(date_tested = NA)

to_fix <- joined_release_2  %>%
  filter(cohort != 'ego-singlechild') %>%
  filter(is.na(date_tested)) %>%
  full_join(wrong_dates_2) %>%
  full_join(wrong_dates) %>%
  mutate(date_range_inferred = str_split_fixed(filename, '_', 5)[,3]) %>%
  mutate(date_inferred = str_split_fixed(date_range_inferred,'-',2)[,1]) %>%
  mutate(date_inferred = mdy(date_inferred)) %>%
  mutate(date_tested = coalesce(date_tested,date_inferred)) %>%
  select(-date_inferred, -date_range_inferred) 


with_new_dates <-  joined_release_2 %>%
  filter(!filename %in% to_fix$filename) %>%
  full_join(to_fix) %>%
  mutate(age_in_days_during_video = as.numeric(difftime(ymd(date_tested), mdy(birthdate), units='days'))) %>%
  select(-birthdate)

```

```{r}
merged <- with_new_dates %>%
  filter(!is.na(subject_id))%>%
  filter(is.na(exclude)) %>%
  mutate(age_in_months = age_in_days_during_video/30.44) %>%
  rename(duration = Duration) %>%
  distinct(filename, subject_id, video_id, duration, num_lang, cohort, video_name, filename, age_in_months, date_tested)


library(assertthat)
assert_that(sum(merged$age_in_months<0)==0)
assert_that(sum(merged$age_in_months>50)==0)
```


```{r}
write_csv(merged, file=here::here('data/merged_data_descriptives.csv'))
```

```{r}
summary_by_cohort <- merged %>%
  filter(!is.na(subject_id))  %>%
  group_by(cohort) %>%
  summarize(num_files = length(unique(filename))) %>%
  kable()
```

```{r}
no_merge <- joined_release_2 %>%
  filter(is.na(subject_id))  %>%
  select(filename)

write_csv(no_merge, file=here::here('data/no_merge_vids.csv'))
```

```{r}

duplicates = c('00320001_GX010058_11.04.2024-11.10.2024_11.09.2024-5:32pm',
'00320001_GX020058_11.04.2024-11.10.2024_11.09.2024-5:32pm',
'00240001_GX010053_04.29.2024-05.05.2024_05.05.2024-NA',
'00370002_GX010087_08.26.2024-09.01.2024_08.31.2024-8:57am',
'00510002_GX010013_09.30.2024-10.06.2024_09.30.2024-6:32am',
'00590001_GX010036_09.16.2024-09.22.2024_09.17.2024-NA')
```

We have `r length(unique(merged$filename))` matched videos out of the `r length(unique(joined_release_2$filename))` that were in the pull of the dataset. We are still missing information about `r  length(unique(no_merge$filename))` files,  we excluded `r sum(!is.na(joined_release_2$exclude))` videos that had sensitive information. We had `r length(joined_release_2$filename)` files and with `r length(unique(joined_release_2$filename))` unique filenames, which means there were a few accidental duplicate videos.  

## Save
```{R}
write_csv(merged, file=here::here('data/merged_dataset_with_descriptives.csv'))
```



## Make a list of the files we need to make sure not to include
```{R}
to_delete_release2 <- joined_release_2 %>%
  filter(!is.na(exclude)) %>%
  select(filename, subject_id, date_tested, Duration, cohort, exclude) %>%
  group_by(cohort) 

# these videos were recorded but then manually deleted and are not in the release or in the raw/storage buckets
to_delete_from_spreadhseet <- all_sessions %>%
  filter(!is.na(exclude)) %>%
  anti_join(to_delete_release2 %>% rename(vid_name = filename))

# these were given to the team before merging was done,
blackout_first_round = read_csv(file=here::here('data/to_delete_blackout_vids.csv')) %>%
  mutate(first_blackout = TRUE)

# here are all the files that need to be deleted
to_delete_release2 <- joined_release_2 %>%
  filter(!is.na(exclude)) %>%
  select(filename, subject_id, date_tested, Duration, cohort, exclude) %>%
  left_join(blackout_first_round)
  
write_csv(to_delete_release2, file=here::here('data/to_delete_blackout_vids_post_match.csv'))
```

# Subsample videos for whisper transcripts 

Get the videos that we manually annotated, do some annoying joining because the video names have changed
```{R}
sampled_release_1 <- read_csv(file=here::here('data/to_sample_for_whisper_may2024.csv')) %>%
  select(filename) %>%
  mutate(filename = str_split_fixed(filename, '.MP4',2)[,1]) %>%
  mutate(subject_id = str_split_fixed(filename, '_',4)[,1]) %>%
  mutate(video_id = str_split_fixed(filename, '_',4)[,2]) %>%
  rename(release_1_filename = filename) %>%
  left_join(merged, by=c('subject_id','video_id'))
```

There are 19 videos that we can't match, but that's not too bad in the scheme of things. Some were luna, some we matched manually to the may 2024 filenames, and two we couldn't figure out.
```{r}
manually_merged_whisper = read_csv(file=here::here('data/manually_merged_whisper.csv'))
```

# Whisper video sampling
To sample videos for Whisper validation, we took a stratified sampling approach subsetted to monolingual, English speaking families. For each subject, we then sampled videos that were 5 minutes or longer, and extracted a thirty-second clip from the middle of the video.
```{r}
release_1_whisper <- sampled_release_1 %>%
  filter(!is.na(filename)) %>% # 19 couldn't match
  filter(num_lang==1) %>%
  group_by(subject_id) %>%
  summarize(num_vids = length(unique(filename)))

release_1_whisper_by_vid <- sampled_release_1 %>%
  filter(!is.na(filename)) %>% # 19 couldn't match
  full_join(manually_merged_whisper) %>%
  filter(num_lang==1) 

```

```{r}
write_csv(sampled_release_1, file=here::here('data/whisper_matched_release_1.csv'))
```

```{R}
# Summarize data to count unique filenames per subject and age bin
plot_data <- release_1_whisper_by_vid %>%
  group_by(subject_id, age_in_months) %>%
  summarise(unique_filenames = n_distinct(filename), .groups = "drop")

# Create the plot
ggplot(plot_data, aes(x = age_in_months, y = unique_filenames, col = subject_id)) +
  geom_col() +  # Use geom_col() instead of geom_bar(stat="identity")
  labs(x = "Age in Months", y = "Unique Filenames", title = "Unique Videos per Age") +
  theme_minimal()
```

## Get videos to annotate
And then save out the csv
```{R}
 to_sample <- merged %>%
  filter(duration>(3*60)) %>% # only videos 3 mins or longer
  filter(cohort!="ego_single") %>% # not luna
  filter(num_lang==1) %>% # monolingual
  filter(!subject_id %in% release_1_whisper$subject_id) %>% # haven't sampled before
   group_by(subject_id) %>%
   arrange(age_in_months) %>%
   mutate(vid_index = row_number()) %>%
   filter(vid_index%%6==0) %>%
   mutate(start_time = duration/2) %>%
   mutate(end_time = start_time + 30) %>%
   arrange(subject_id, age_in_months)

write_csv(to_sample, file=here::here('data/to_sample_for_whisper_release2.csv'))
```




